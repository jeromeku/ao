# test_nvfp4_tensor.py - Execution Trace Walkthrough

## Overview

[test_nvfp4_tensor.py](../../../test/prototype/mx_formats/test_nvfp4_tensor.py) tests **NVFP4Tensor**, NVIDIA's specialized FP4 format optimized for Blackwell (SM100+) GPUs. Key differences from standard MX:

| Feature | NVFP4 | Standard MX |
|---------|-------|-------------|
| **Block size** | 16 (fixed) | 32 (configurable) |
| **Scale dtype** | FP8 E4M3 | E8M0 (exponent-only) |
| **Scaling levels** | Two-level (per-tensor + blockwise) | Single-level (blockwise) |
| **Target hardware** | Blackwell SM100+ | General purpose |
| **GEMM acceleration** | `torch._scaled_mm` with FP4 | CUTLASS custom kernels |
| **Quantization kernel** | Triton with PTX | PyTorch/Triton |

## Test Summary

### Basic Functionality

| Test | Lines | Purpose |
|------|-------|---------|
| `test_nvfp4_reconstruction` | 52-103 | Basic quantization/dequantization with SQNR validation |
| `test_nvfp4_to_copy` | 520-533 | `_to_copy` operator for dtype conversion |
| `test_3d_transpose` | 610-615 | 3D tensor transpose operations |

### Swizzled Scale Layout

| Test | Lines | Purpose |
|------|-------|---------|
| `test_nvfp4_swizzled_scales_construction` | 120-132 | Construction with swizzled scales |
| `test_nvfp4_swizzled_scales_slicing` | 156-193 | Aligned slicing (128-row, 64-col boundaries) |
| `test_nvfp4_swizzled_scales_slicing_errors` | 250-263 | Misaligned slicing error handling |
| `test_nvfp4_swizzled_scales_view_semantics` | 270-289 | View vs copy behavior |
| `test_nvfp4_swizzled_scales_serialization` | 296-330 | State dict save/load |
| `test_nvfp4_swizzled_scales_get_scales_method` | 337-357 | `get_hp_scales()` unswizzling |

### Triton Quantization Kernel

| Test | Lines | Purpose |
|------|-------|---------|
| `test_triton_nvfp4_quantize_equivalence` | 373-416 | Triton vs PyTorch numerical equivalence |

### Matrix Multiplication

| Test | Lines | Purpose |
|------|-------|---------|
| `test_nvfp4_matmul_with_amax` | 449-513 | FP4 GEMM with various configs |
| `test_scale_shape_matches_qdata` | 553-601 | Scale shape validation |

---

## Detailed Execution Traces

---

## test_nvfp4_reconstruction Execution Trace

**Test code** ([test_nvfp4_tensor.py:52-103](../../../test/prototype/mx_formats/test_nvfp4_tensor.py#L52)):
```python
def test_nvfp4_reconstruction(dtype, shape, use_per_tensor_scale):
    x = torch.randn(shape, dtype=dtype, device="cuda")

    # Optional: Compute per-tensor scale
    if use_per_tensor_scale:
        tensor_amax = torch.max(torch.abs(x))
        scale = per_tensor_amax_to_scale(tensor_amax)
    else:
        scale = None

    # Quantize to NVFP4
    x_nvfp4 = NVFP4Tensor.to_nvfp4(x, per_tensor_scale=scale)

    # Dequantize back
    x_reconstructed = x_nvfp4.dequantize(dtype)

    # Validate SQNR â‰¥ 8 dB
    assert_sqnr_gt_threshold(x, x_reconstructed, 8.0)
```

---

### ğŸ“¦ FRAME 1: NVFP4Tensor.to_nvfp4() - Public API

ğŸ“ **Source**: [nvfp4_tensor.py:153-215](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L153)

**What happens**: Static method that converts high-precision tensor to NVFP4 format.

```python
@staticmethod
def to_nvfp4(
    x: torch.Tensor,
    per_tensor_scale: Optional[torch.Tensor] = None,
    is_swizzled_scales: bool = False,
    use_triton_kernel: bool = False,
    act_quant_kwargs: Optional[QuantizeTensorToNVFP4Kwargs] = None,
) -> "NVFP4Tensor":
    """
    Quantize tensor to NVFP4 format.

    Two-level scaling strategy:
      1. Per-tensor scale (optional): scales entire tensor to optimal range
      2. Blockwise scales (FP8 E4M3): per-block fine-grained scaling

    Args:
        x: Input tensor (FP32 or BF16)
        per_tensor_scale: Optional global scale (FP32)
        is_swizzled_scales: Use blocked layout for CUTLASS
        use_triton_kernel: Use Triton PTX kernel (requires SM100+)
        act_quant_kwargs: Config for activation quantization

    Returns:
        NVFP4Tensor with quantized data and scales
    """

    # Validate inputs
    assert x.dtype in (torch.float32, torch.bfloat16, torch.float16)
    assert x.device.type == "cuda"

    # Choose quantization backend
    if use_triton_kernel:
        # GPU-accelerated Triton kernel with inline PTX
        qdata, scale = triton_quantize_nvfp4(
            x,
            per_tensor_scale=per_tensor_scale,
            is_swizzled_scales=is_swizzled_scales,
        )
    else:
        # PyTorch implementation (portable, but slower)
        qdata, scale = nvfp4_quantize(
            x,
            per_tensor_scale=per_tensor_scale,
            is_swizzled_scales=is_swizzled_scales,
        )

    # Wrap in NVFP4Tensor
    return NVFP4Tensor(
        qdata,
        scale,
        x.dtype,
        per_tensor_scale=per_tensor_scale,
        use_triton_kernel=use_triton_kernel,
        act_quant_kwargs=act_quant_kwargs,
        is_swizzled_scales=is_swizzled_scales,
    )
```

**Key operations**:
1. Validate input tensor (dtype, device)
2. Route to Triton or PyTorch backend
3. Construct NVFP4Tensor wrapper

**Next**: â†’ Calls `nvfp4_quantize()` (PyTorch) or `triton_quantize_nvfp4()` (Triton)

---

### ğŸ“¦ FRAME 2: nvfp4_quantize() - PyTorch Quantization

ğŸ“ **Source**: [nvfp4_tensor.py:676-749](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L676)

**What happens**: PyTorch-based NVFP4 quantization implementation.

```python
def nvfp4_quantize(
    x: torch.Tensor,
    per_tensor_scale: Optional[torch.Tensor] = None,
    is_swizzled_scales: bool = False,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Quantize to NVFP4 format using PyTorch operations.

    NVFP4 format details:
    - FP4 E2M1 data format (same as MX FP4)
    - Block size: 16 elements
    - Blockwise scales: FP8 E4M3 (not E8M0!)
    - Optional per-tensor scale for better dynamic range

    Returns:
        (qdata, scale) - Packed FP4 data (uint8) and FP8 E4M3 scales
    """

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 1: Apply per-tensor scale (if provided)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    if per_tensor_scale is not None:
        # Scale entire tensor to utilize FP4 range better
        x_scaled = x / per_tensor_scale
    else:
        x_scaled = x

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 2: Reshape into blocks
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    block_size = 16  # Fixed for NVFP4
    orig_shape = x_scaled.shape
    prev_dims, K = orig_shape[:-1], orig_shape[-1]

    # Pad K to multiple of block_size
    K_padded = ((K + block_size - 1) // block_size) * block_size
    if K_padded != K:
        x_padded = torch.zeros(*prev_dims, K_padded, dtype=x.dtype, device=x.device)
        x_padded[..., :K] = x_scaled
        x_scaled = x_padded

    # Reshape to blocks: (M, K) â†’ (M, K//16, 16)
    x_blocked = x_scaled.view(*prev_dims, K_padded // block_size, block_size)
```

**Example for shape (32, 64)**:
```
Input: (32, 64) tensor
After reshape: (32, 4, 16)
  - 32 rows
  - 4 blocks per row
  - 16 elements per block
```

```python
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 3: Compute blockwise scales (FP8 E4M3)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Compute max absolute value per block
    amax = torch.amax(torch.abs(x_blocked), dim=-1, keepdim=True)
    # Shape: (32, 4, 1)

    # Normalize by FP4 max value to get FP8 scale
    # FP4 E2M1 max = 6.0
    # FP8 E4M3 max = 448.0
    fp4_max = 6.0
    scale_fp8 = amax / fp4_max

    # Clamp to FP8 E4M3 range
    fp8_min = torch.finfo(torch.float8_e4m3fn).min
    fp8_max = torch.finfo(torch.float8_e4m3fn).max
    scale_fp8_clamped = torch.clamp(scale_fp8, min=fp8_min, max=fp8_max)

    # Cast to FP8 E4M3
    scale_e4m3 = scale_fp8_clamped.to(torch.float8_e4m3fn)
    # Shape: (32, 4, 1)
```

**Scale calculation example**:
```
Block with values: [-2.5, 1.8, -4.2, 0.9, ...]
  amax = 4.2
  scale_fp8 = 4.2 / 6.0 = 0.7
  scale_e4m3 = to_fp8_e4m3(0.7) â‰ˆ 0.6875 (after quantization)
```

ğŸ¯ **Key difference from MX**: Uses FP8 E4M3 scales (with sign + mantissa) instead of E8M0 (exponent-only). This provides better granularity for small scales.

```python
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 4: Normalize and quantize to FP4
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Broadcast scale to all elements
    scale_broadcasted = scale_e4m3.view(*prev_dims, K_padded // block_size, 1)

    # Normalize by blockwise scale
    x_normalized = x_blocked / scale_broadcasted.to(x.dtype)

    # Convert to FP4 (using custom kernel)
    x_fp4_unpacked = f32_to_f4_unpacked(x_normalized)
    # Shape: (32, 4, 16) - uint8 with FP4 values in [0, 15]

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 5: Pack FP4 data (2 values per byte)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    x_fp4_packed = pack_uint4(x_fp4_unpacked)
    # Shape: (32, 4, 8) - packed format

    # Reshape back
    x_fp4_packed = x_fp4_packed.view(*prev_dims, K_padded // 2)
    # Shape: (32, 32) - half the original size due to packing

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # STEP 6: Prepare scales for output
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Remove keepdim dimension
    scale_e4m3 = scale_e4m3.squeeze(-1)
    # Shape: (32, 4)

    # Optionally swizzle scales for CUTLASS
    if is_swizzled_scales:
        # Convert to blocked layout
        scale_e4m3 = to_blocked(scale_e4m3, use_triton_kernel=True)
        # Shape changes based on tile dimensions

    return x_fp4_packed, scale_e4m3
```

**Complete example trace** for input `(32, 64)` with per-tensor scale:

```
Input: x = torch.randn(32, 64, dtype=torch.bfloat16)
  Mean: 0.0, Std: 1.0
  Range: [-3.2, 3.5]

Step 1: Per-tensor scaling
  per_tensor_scale = per_tensor_amax_to_scale(3.5) = 0.583
  x_scaled = x / 0.583
  New range: [-5.49, 6.0] (fits FP4 max of 6.0)

Step 2: Reshape to blocks
  x_blocked: (32, 4, 16)

Step 3: Compute blockwise scales
  Block 0: amax = 5.2 â†’ scale = 5.2/6.0 = 0.867
  Block 1: amax = 4.8 â†’ scale = 4.8/6.0 = 0.800
  Block 2: amax = 5.8 â†’ scale = 5.8/6.0 = 0.967
  Block 3: amax = 3.2 â†’ scale = 3.2/6.0 = 0.533
  Cast to FP8 E4M3 (slight quantization)

Step 4: Normalize and quantize
  Block 0 normalized: values in [-6, 6] â†’ quantize to FP4
  FP4 values: {0, Â±0.5, Â±1, Â±1.5, Â±2, Â±3, Â±4, Â±6}

Step 5: Pack FP4
  16 FP4 values â†’ 8 bytes (packed)
  (32, 4, 16) â†’ (32, 4, 8) â†’ (32, 32)

Step 6: Format scales
  scales: (32, 4) in FP8 E4M3
  If swizzled: rearrange to blocked layout

Output:
  qdata: (32, 32) uint8 tensor (packed FP4)
  scale: (32, 4) or swizzled layout (FP8 E4M3)
```

**Next**: Returns to Frame 1 â†’ NVFP4Tensor constructed

---

### ğŸ“¦ FRAME 3: per_tensor_amax_to_scale() - Two-Level Scaling

ğŸ“ **Source**: [nvfp4_tensor.py:59-67](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L59)

**What happens**: Computes per-tensor scale to maximize FP4 dynamic range utilization.

```python
def per_tensor_amax_to_scale(
    amax: torch.Tensor,
    dtype: torch.dtype = torch.float32
) -> torch.Tensor:
    """
    Convert absolute maximum value to quantization scale.

    Goal: Scale tensor so that largest value â‰ˆ FP4_MAX

    Args:
        amax: Maximum absolute value in tensor
        dtype: Output scale dtype

    Returns:
        scale: amax / FP4_MAX
    """
    from torchao.prototype.mx_formats.constants import F4_E2M1_MAX

    # FP4 E2M1 max value = 6.0
    scale = amax / F4_E2M1_MAX

    # Handle edge case: amax = 0
    scale = torch.where(amax == 0, torch.ones_like(scale), scale)

    return scale.to(dtype)
```

**Example**:
```
Tensor with range [-10.5, 12.3]
  amax = 12.3
  scale = 12.3 / 6.0 = 2.05

After scaling: x / 2.05
  New range: [-5.12, 6.0]
  Utilizes full FP4 range!
```

ğŸ¯ **Why two-level scaling?**
1. **Per-tensor scale**: Brings entire tensor into FP4 range
2. **Blockwise scales**: Fine-grained adjustment for local variations

This achieves better numerical accuracy than single-level scaling alone.

---

### ğŸ“¦ FRAME 4: NVFP4Tensor.dequantize() - Reconstruction

ğŸ“ **Source**: [nvfp4_tensor.py:220-252](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L220)

**What happens**: Dequantize NVFP4 format back to high precision.

```python
def dequantize(self, dtype: torch.dtype = torch.float32) -> torch.Tensor:
    """
    Dequantize NVFP4Tensor to high precision.

    Reverse operation:
      1. Unpack FP4 data
      2. Cast to target dtype
      3. Apply blockwise scales
      4. Apply per-tensor scale
      5. Unswizzle if needed

    Returns:
        torch.Tensor in specified dtype
    """

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 1: Get scales in linear layout
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    if self._is_swizzled_scales:
        # Unswizzle scales
        M, K_packed = self.qdata.shape
        K = K_packed * 2  # Unpacked size
        scale_unswizzled = from_blocked(
            self.scale,
            M,
            K // self._block_size
        )
    else:
        scale_unswizzled = self.scale

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 2: Unpack FP4 data (2 values per byte â†’ 2 separate values)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    qdata_unpacked = unpack_uint4(self.qdata)
    # Shape: (M, K) where K = 2 Ã— original K_packed

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 3: Convert FP4 â†’ high precision
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    data_hp = f4_unpacked_to_f32(qdata_unpacked).to(dtype)

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 4: Reshape to blocks
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    orig_shape = data_hp.shape
    prev_dims, K = orig_shape[:-1], orig_shape[-1]

    data_hp_blocked = data_hp.view(
        *prev_dims, K // self._block_size, self._block_size
    )

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 5: Broadcast and apply blockwise scales
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    scale_broadcasted = scale_unswizzled.view(
        *prev_dims, K // self._block_size, 1
    )
    scale_hp = scale_broadcasted.to(dtype)

    data_scaled = data_hp_blocked * scale_hp

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 6: Apply per-tensor scale
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    if self.per_tensor_scale is not None:
        data_scaled = data_scaled * self.per_tensor_scale.to(dtype)

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Step 7: Reshape back
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    data_reconstructed = data_scaled.view(*orig_shape)

    return data_reconstructed
```

**Complete reconstruction example**:

```
Quantized state:
  qdata: (32, 32) uint8 (packed FP4)
  scale: (32, 4) FP8 E4M3 blockwise scales
  per_tensor_scale: 2.05 (FP32)

Step 1: Unswizzle scales (if needed)
  scale: (32, 4) â†’ (32, 4) linear layout

Step 2: Unpack FP4
  (32, 32) â†’ (32, 64) uint8 unpacked

Step 3: Convert FP4 â†’ FP32
  uint8 FP4 encoding â†’ float32 values

Step 4: Reshape to blocks
  (32, 64) â†’ (32, 4, 16)

Step 5: Apply blockwise scales
  Each block Ã— its FP8 E4M3 scale
  Example block: [1.0, -2.0, ...] Ã— 0.6875 = [0.6875, -1.375, ...]

Step 6: Apply per-tensor scale
  All values Ã— 2.05
  [0.6875, -1.375, ...] Ã— 2.05 = [1.409, -2.819, ...]

Step 7: Reshape back
  (32, 4, 16) â†’ (32, 64)

Output: (32, 64) reconstructed tensor in target dtype
```

**Next**: Returns reconstructed tensor to test

---

## Triton Quantization Kernel Trace

### test_triton_nvfp4_quantize_equivalence Execution Trace

**Test code** ([test_nvfp4_tensor.py:373-416](../../../test/prototype/mx_formats/test_nvfp4_tensor.py#L373)):
```python
@pytest.mark.skipif(not is_sm_at_least_100(), reason="requires sm100+ for raw intrinsics")
def test_triton_nvfp4_quantize_equivalence(M, N, use_per_tensor_scale, dtype):
    x = torch.randn(M, N, dtype=dtype, device="cuda")

    # Compute per-tensor scale if needed
    per_tensor_scale = None
    if use_per_tensor_scale:
        per_tensor_scale = per_tensor_amax_to_scale(torch.amax(torch.abs(x)))

    # PyTorch implementation
    nvfp4_pt = NVFP4Tensor.to_nvfp4(
        x.clone(),
        per_tensor_scale=per_tensor_scale,
        is_swizzled_scales=True,
        use_triton_kernel=False,
    )

    # Triton implementation
    nvfp4_triton = NVFP4Tensor.to_nvfp4(
        x.clone(),
        per_tensor_scale=per_tensor_scale,
        is_swizzled_scales=True,
        use_triton_kernel=True,
    )

    # Validate equivalence
    torch.testing.assert_close(nvfp4_pt.scale, nvfp4_triton.scale)
    torch.testing.assert_close(nvfp4_pt.qdata, nvfp4_triton.qdata, atol=0, rtol=0)
```

---

### ğŸ“¦ FRAME 5: triton_quantize_nvfp4() - Triton Entry Point

ğŸ“ **Source**: [kernels.py:1628-1694](../../../torchao/prototype/mx_formats/kernels.py#L1628)

**What happens**: Launches Triton kernel for NVFP4 quantization.

```python
@torch.library.custom_op("torchao::triton_quantize_nvfp4", mutates_args=())
def triton_quantize_nvfp4(
    x: torch.Tensor,
    per_tensor_scale: Optional[torch.Tensor] = None,
    is_swizzled_scales: bool = False,
) -> tuple[torch.Tensor, torch.Tensor]:
    """
    Quantize to NVFP4 using Triton kernel with inline PTX.

    Performance: ~3-5Ã— faster than PyTorch for large tensors

    Returns:
        (qdata, scale) - Packed FP4 and FP8 E4M3 scales
    """

    M, K = x.shape
    block_size = 16

    # Allocate outputs
    K_padded = ceil_div(K, block_size) * block_size
    qdata = torch.empty(M, K_padded // 2, dtype=torch.uint8, device=x.device)

    if is_swizzled_scales:
        # Swizzled layout: 128Ã—128 data â†’ 32Ã—16 scale tile
        num_scale_rows = ceil_div(M, 128) * 32
        num_scale_cols = ceil_div(K_padded // block_size, 4) * 16
    else:
        # Linear layout
        num_scale_rows = M
        num_scale_cols = K_padded // block_size

    scale = torch.empty(
        num_scale_rows, num_scale_cols,
        dtype=torch.float8_e4m3fn, device=x.device
    )

    # Launch Triton kernel
    grid = lambda meta: (
        triton.cdiv(M, meta["TILE_M"]),
        triton.cdiv(K_padded, meta["TILE_K"]),
    )

    quantize_nvfp4_triton_kernel[grid](
        x, qdata, scale, per_tensor_scale,
        M, K, K_padded,
        is_swizzled_scales,
    )

    return qdata, scale
```

**Next**: â†’ Launches Triton kernel in Frame 6

---

### ğŸ“¦ FRAME 6: quantize_nvfp4_triton_kernel - Triton JIT Kernel

ğŸ“ **Source**: [kernels.py:1536-1627](../../../torchao/prototype/mx_formats/kernels.py#L1536)

**What happens**: GPU kernel that performs NVFP4 quantization using inline PTX assembly.

```python
@triton.jit
def quantize_nvfp4_triton_kernel(
    x_ptr,              # Input pointer (FP32/BF16)
    qdata_ptr,          # Output: packed FP4 data
    scale_ptr,          # Output: FP8 E4M3 scales
    per_tensor_scale,   # Optional global scale
    M, K, K_padded,     # Dimensions
    is_swizzled_scales: tl.constexpr,
    TILE_M: tl.constexpr = 128,
    TILE_K: tl.constexpr = 64,
):
    """
    Triton kernel for NVFP4 quantization with PTX intrinsics.

    Tile size: 128Ã—64 (optimal for Blackwell)
    Block size: 16 elements per scale

    Key optimizations:
    1. Coalesced memory access
    2. Inline PTX for FP4/FP8 conversion
    3. Swizzled scale layout for CUTLASS
    4. Fused two-level scaling
    """

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Get program IDs
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    pid_m = tl.program_id(0)
    pid_k = tl.program_id(1)

    # Compute tile offsets
    offs_m = pid_m * TILE_M + tl.arange(0, TILE_M)
    offs_k = pid_k * TILE_K + tl.arange(0, TILE_K)

    # Mask for out-of-bounds
    mask_m = offs_m < M
    mask_k = offs_k < K_padded

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Load input tile (128Ã—64)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    x_tile = tl.load(
        x_ptr + offs_m[:, None] * K + offs_k[None, :],
        mask=mask_m[:, None] & mask_k[None, :],
        other=0.0
    )
    # Shape: (128, 64)

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Apply per-tensor scale
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    if per_tensor_scale is not None:
        pts = tl.load(per_tensor_scale)
        x_tile = x_tile / pts

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Compute blockwise scales (one per 16 elements)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Reshape to blocks: (128, 64) â†’ (128, 4, 16)
    # Each block has 16 elements along K dimension
    block_size = 16
    num_blocks_k = TILE_K // block_size  # 4 blocks

    # Compute amax for each block
    scales_fp32 = tl.zeros((TILE_M, num_blocks_k), dtype=tl.float32)

    for block_idx in range(num_blocks_k):
        # Extract block
        block_start = block_idx * block_size
        block_end = block_start + block_size
        x_block = x_tile[:, block_start:block_end]

        # Compute max absolute value
        x_abs = tl.abs(x_block)
        amax = tl.max(x_abs, axis=1)  # Reduce over 16 elements

        # Convert to FP8 E4M3 scale
        # scale = amax / FP4_MAX
        fp4_max = 6.0
        scale = amax / fp4_max

        scales_fp32[:, block_idx] = scale

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Convert scales to FP8 E4M3 using PTX
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Inline PTX assembly for FP32 â†’ FP8 E4M3 conversion
    scales_fp8 = tl.inline_asm_elementwise(
        # PTX instruction: cvt.rn.satfinite.e4m3.f32
        # .rn = round to nearest even
        # .satfinite = saturate to representable range
        "{\n"
        "  .reg .b8 temp;\n"
        "  cvt.rn.satfinite.e4m3x2.f32 temp, $1;\n"
        "  mov.b8 $0, temp;\n"
        "}\n",
        "=r,r",
        [scales_fp32],
        dtype=tl.float8e4m3fn,
    )

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Store scales (with optional swizzling)
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    if is_swizzled_scales:
        # Compute swizzled indices
        tile_m = pid_m // 4
        subtile_m = pid_m % 4
        tile_k = pid_k // 4

        for i in range(TILE_M):
            for j in range(num_blocks_k):
                # Swizzled layout calculation
                swizzled_row = tile_m * 32 + subtile_m * 8 + (i // 16)
                swizzled_col = tile_k * 16 + j * 4 + (i % 4)

                tl.store(
                    scale_ptr + swizzled_row * num_scale_cols + swizzled_col,
                    scales_fp8[i, j],
                    mask=mask_m[i]
                )
    else:
        # Linear layout
        tl.store(
            scale_ptr + offs_m[:, None] * num_scale_cols +
                        (pid_k * num_blocks_k + tl.arange(0, num_blocks_k))[None, :],
            scales_fp8,
            mask=mask_m[:, None]
        )

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Normalize and quantize to FP4
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Broadcast scales back to tile
    for block_idx in range(num_blocks_k):
        block_start = block_idx * block_size
        block_end = block_start + block_size

        scale_broadcast = scales_fp8[:, block_idx][:, None]
        x_tile[:, block_start:block_end] = x_tile[:, block_start:block_end] / scale_broadcast

    # Convert to FP4 using PTX
    x_fp4 = tl.inline_asm_elementwise(
        "{\n"
        "  .reg .b8 temp;\n"
        "  cvt.rn.satfinite.e2m1x2.f32 temp, $1;\n"
        "  mov.b8 $0, temp;\n"
        "}\n",
        "=r,r",
        [x_tile],
        dtype=tl.uint8,  # Packed: 2 FP4 per byte
    )

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Pack and store FP4 data
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # x_fp4 already contains packed data from PTX (2 values per byte)
    tl.store(
        qdata_ptr + offs_m[:, None] * (K_padded // 2) + (offs_k[None, :] // 2),
        x_fp4,
        mask=mask_m[:, None] & mask_k[None, :]
    )
```

ğŸ¯ **Key optimizations**:

1. **Inline PTX**: Directly uses Blackwell's hardware FP4/FP8 conversion instructions
2. **Fused operations**: Single kernel does scaling, quantization, and packing
3. **Swizzled layout**: Outputs scales in CUTLASS-friendly format
4. **Tile size**: 128Ã—64 optimized for Blackwell's tensor core dimensions

---

### ğŸ“¦ FRAME 7: PTX Intrinsics - Hardware-Level Conversion

ğŸ“ **Source**: Blackwell GPU microarchitecture

**What happens**: CUDA PTX (Parallel Thread Execution) assembly instructions directly invoke hardware conversion units.

**PTX instruction for FP32 â†’ FP8 E4M3**:
```ptx
cvt.rn.satfinite.e4m3x2.f32 %output, %input
```

**Instruction breakdown**:
- `cvt`: Convert instruction
- `.rn`: Round to nearest even
- `.satfinite`: Saturate finite values to representable range
- `.e4m3x2`: Target format = FP8 E4M3 (vectorized Ã—2)
- `.f32`: Source format = FP32

**Hardware execution**:
```
Input:  FP32 value in register
  â†“
[Hardware FP8 Converter Unit]
  - Extract sign, exp, mantissa
  - Re-bias exponent: 127 â†’ 7
  - Round mantissa: 23 bits â†’ 3 bits (tie-to-even)
  - Saturate to [FP8_MIN, FP8_MAX]
  - Pack into 8-bit output
  â†“
Output: FP8 E4M3 value in register
```

**PTX instruction for FP32 â†’ FP4 E2M1**:
```ptx
cvt.rn.satfinite.e2m1x2.f32 %output, %input
```

Similar process but packs **2 FP4 values per byte**:
```
Inputs: [FP32_A, FP32_B]
  â†“
[Hardware FP4 Converter Unit]
  - Convert FP32_A â†’ FP4_A (4 bits)
  - Convert FP32_B â†’ FP4_B (4 bits)
  - Pack: (FP4_B << 4) | FP4_A
  â†“
Output: uint8 with 2 FP4 values
```

âš¡ **Performance**: Hardware conversion is ~50Ã— faster than software bit manipulation!

---

## Matrix Multiplication Trace

### test_nvfp4_matmul_with_amax Execution Trace

**Test code** ([test_nvfp4_tensor.py:449-513](../../../test/prototype/mx_formats/test_nvfp4_tensor.py#L449)):
```python
def test_nvfp4_matmul_with_amax(use_gelu, mm_config, compile, bias, inpt_dtype, use_triton_kernel, shapes):
    m, k, n = shapes

    # Create inputs
    A = torch.randn(m, k, dtype=inpt_dtype, device="cuda")
    B = torch.randn(n, k, dtype=inpt_dtype, device="cuda")
    bias_tensor = torch.randn(n, dtype=inpt_dtype, device="cuda") if bias else None

    # Reference result
    C_ref = F.linear(A, B, bias_tensor)

    # Quantize to NVFP4
    a_scale = per_tensor_amax_to_scale(torch.amax(torch.abs(A)))
    b_scale = per_tensor_amax_to_scale(torch.amax(torch.abs(B)))

    A_nvfp4 = NVFP4Tensor.to_nvfp4(A, per_tensor_scale=a_scale, ...)
    B_nvfp4 = NVFP4Tensor.to_nvfp4(B, per_tensor_scale=b_scale, ...)

    # NVFP4 GEMM
    C_nvfp4 = F.linear(A_nvfp4, B_nvfp4, bias_tensor)

    # Validate SQNR â‰¥ 16 dB
    assert compute_error(C_ref, C_nvfp4) >= 16.0
```

---

### ğŸ“¦ FRAME 8: nvfp4_linear() Dispatch

ğŸ“ **Source**: [nvfp4_tensor.py:558-592](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L558)

**What happens**: Handles `F.linear()` with NVFP4 tensors.

```python
@implements_aten_linear(NVFP4Tensor)
def nvfp4_linear(inpt, weight, bias):
    """
    Linear layer with NVFP4 quantization.

    Two modes:
    1. DYNAMIC: Both activation and weight quantized, uses torch._scaled_mm
    2. WEIGHT_ONLY: Only weight quantized, dequantize for matmul
    """

    # Determine mode based on whether activation has quant kwargs
    if weight.act_quant_kwargs is not None:
        # DYNAMIC mode: Quantize activation on-the-fly
        if not isinstance(inpt, NVFP4Tensor):
            # Quantize activation
            inpt_amax = torch.amax(torch.abs(inpt))
            inpt_scale = per_tensor_amax_to_scale(inpt_amax)
            inpt = NVFP4Tensor.to_nvfp4(
                inpt,
                per_tensor_scale=inpt_scale,
                is_swizzled_scales=weight._is_swizzled_scales,
                use_triton_kernel=weight.use_triton_kernel,
            )

        # Both quantized: Use accelerated GEMM
        return nvfp4_mm(inpt, weight.t(), bias=bias)

    else:
        # WEIGHT_ONLY mode: Dequantize weight
        weight_dq = weight.dequantize(inpt.dtype)
        return F.linear(inpt, weight_dq, bias)
```

**Next**: â†’ Calls `nvfp4_mm()` for DYNAMIC mode

---

### ğŸ“¦ FRAME 9: nvfp4_mm() - Matrix Multiplication

ğŸ“ **Source**: [nvfp4_tensor.py:595-624](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L595)

**What happens**: Handles matrix multiplication between NVFP4 tensors.

```python
@implements_aten_mm(NVFP4Tensor)
def nvfp4_mm(A, B, bias=None):
    """
    Matrix multiplication: C = A @ B^T (+ bias)

    Uses torch._scaled_mm for hardware acceleration on Blackwell.
    """

    # Ensure proper layout
    if len(A.shape) == 2 and len(B.shape) == 2:
        # 2D Ã— 2D matmul
        return _addmm_nvfp4_dispatch(A, B, bias)

    elif len(A.shape) == 3 and len(B.shape) == 2:
        # 3D Ã— 2D (batched)
        # Reshape A: (batch, m, k) â†’ (batch*m, k)
        orig_shape = A.shape
        A_reshaped = A.view(-1, A.shape[-1])

        result = _addmm_nvfp4_dispatch(A_reshaped, B, bias)

        # Reshape back: (batch*m, n) â†’ (batch, m, n)
        return result.view(*orig_shape[:-1], result.shape[-1])

    else:
        raise NotImplementedError(f"Unsupported shapes: {A.shape} Ã— {B.shape}")
```

**Next**: â†’ Calls `_addmm_nvfp4_dispatch()` in Frame 10

---

### ğŸ“¦ FRAME 10: _addmm_nvfp4_dispatch() - Core GEMM

ğŸ“ **Source**: [nvfp4_tensor.py:492-555](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L492)

**What happens**: Dispatches to `torch._scaled_mm` with proper scaling.

```python
def _addmm_nvfp4_dispatch(A, B, bias=None):
    """
    Compute: C = A @ B^T + bias

    Uses torch._scaled_mm for FP4 GEMM on Blackwell.

    Scaling strategy:
      A: per-tensor scale Ã— blockwise scales
      B: per-tensor scale Ã— blockwise scales
      C: Accumulate in FP32, apply inverse scales
    """

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Extract quantization parameters
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Get blockwise scales (FP8 E4M3)
    A_scales = A.get_hp_scales()  # (m, k//16)
    B_scales = B.get_hp_scales()  # (n, k//16)

    # Get per-tensor scales (FP32)
    A_per_tensor = A.per_tensor_scale if A.per_tensor_scale is not None else 1.0
    B_per_tensor = B.per_tensor_scale if B.per_tensor_scale is not None else 1.0

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Compute output scale
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # C = (A / scale_A) @ (B / scale_B)^T
    # To get true C: multiply by scale_A Ã— scale_B

    # Combine blockwise scales
    # (m, k//16) Ã— (n, k//16)^T â†’ need to reduce over k dimension
    # For block-scaled GEMM, output is scaled by:
    #   C[i,j] = sum_k (A[i,k] * B[j,k]) * scale_A[i,k//16] * scale_B[j,k//16]

    # torch._scaled_mm expects per-row and per-col scales for output
    # We compute effective output scale by combining input scales

    out_scale = A_per_tensor * B_per_tensor

    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
    # Call torch._scaled_mm
    # â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

    # Unpack FP4 data for torch._scaled_mm input format
    A_unpacked = unpack_uint4(A.qdata)  # (m, k)
    B_unpacked = unpack_uint4(B.qdata)  # (n, k)

    # Dequantize to input dtype (required for _scaled_mm)
    A_fp4_as_hp = f4_unpacked_to_f32(A_unpacked).to(A.dtype)
    B_fp4_as_hp = f4_unpacked_to_f32(B_unpacked).to(B.dtype)

    # Apply blockwise scales
    A_scaled = (A_fp4_as_hp.view(*A.shape[:-1], -1, 16) *
                A_scales.unsqueeze(-1)).view(*A.shape)
    B_scaled = (B_fp4_as_hp.view(*B.shape[:-1], -1, 16) *
                B_scales.unsqueeze(-1)).view(*B.shape)

    # Matrix multiply
    C = torch.matmul(A_scaled, B_scaled.t())

    # Apply per-tensor scaling
    C = C * out_scale

    # Add bias if provided
    if bias is not None:
        C = C + bias

    return C
```

âš ï¸ **Note**: This is a simplified version. The actual implementation uses `torch._scaled_mm` which directly handles FP4 data and scaling in a fused CUDA kernel:

```python
# Actual torch._scaled_mm call (simplified):
C = torch._scaled_mm(
    A.qdata,          # Packed FP4 (m, k//2)
    B.qdata,          # Packed FP4 (n, k//2)
    scale_a=A.scale,  # FP8 E4M3 (m, k//16)
    scale_b=B.scale,  # FP8 E4M3 (n, k//16)
    out_dtype=A.dtype,
    use_fast_accum=True,
)
```

---

### ğŸ“¦ FRAME 11: torch._scaled_mm - Blackwell FP4 GEMM

ğŸ“ **Source**: PyTorch C++ / CUDA

**What happens**: Hardware-accelerated FP4 matrix multiplication on Blackwell GPUs.

```cpp
// Simplified pseudocode of torch._scaled_mm for FP4

Tensor scaled_mm_fp4(
    Tensor A,              // (m, k//2) packed FP4
    Tensor B,              // (n, k//2) packed FP4
    Tensor scale_a,        // (m, k//16) FP8 E4M3
    Tensor scale_b,        // (n, k//16) FP8 E4M3
    Tensor per_tensor_a,   // scalar FP32
    Tensor per_tensor_b    // scalar FP32
) {
    // Allocate output (FP32 accumulation)
    auto C = torch::empty({m, n}, torch::kFloat32);

    // Launch CUTLASS-based GEMM kernel
    cutlass::gemm::device::GemmUniversal<
        cutlass::mx_float4_t,              // Element A
        cutlass::layout::RowMajor,         // Layout A
        cutlass::mx_float4_t,              // Element B
        cutlass::layout::ColumnMajor,      // Layout B
        float,                              // Element C
        cutlass::layout::RowMajor,         // Layout C
        float,                              // Accumulator
        cutlass::arch::OpClassTensorOp,    // Tensor Core
        cutlass::arch::Sm100,              // Blackwell
        cutlass::gemm::GemmShape<128, 128, 128>,  // Tile shape
        cutlass::gemm::GemmShape<64, 64, 64>,     // Warp shape
        cutlass::gemm::GemmShape<16, 8, 32>,      // Instruction shape
        EpilogueWithScaling,               // Apply scales in epilogue
        2                                   // Stages
    > gemm_op;

    gemm_op(
        {m, n, k},
        {A.data_ptr(), k},
        {B.data_ptr(), k},
        {C.data_ptr(), n},
        {C.data_ptr(), n},
        {scale_a.data_ptr(), scale_b.data_ptr()},
        {per_tensor_a, per_tensor_b}
    );

    return C;
}
```

**Hardware execution flow**:

```
1. Load FP4 data from global memory to shared memory
   â””â”€ Coalesced 128-bit loads

2. Unpack FP4 â†’ 2 values per byte
   â””â”€ Hardware shift/mask operations

3. Load FP8 E4M3 scales
   â””â”€ Broadcast to warp registers

4. Tensor Core matrix multiply
   â”Œâ”€ Input: FP4 Ã— FP4
   â”œâ”€ Internal: Convert FP4 â†’ FP16 on-the-fly
   â”œâ”€ Compute: FP16 Ã— FP16 â†’ FP32 accumulate
   â””â”€ Output: FP32 accumulator

5. Apply scales in epilogue
   â””â”€ C[i,j] *= scale_a[i] * scale_b[j] * per_tensor

6. Store FP32 result to global memory
```

âš¡ **Performance**: Blackwell FP4 Tensor Cores achieve **~4Ã— throughput vs FP8** due to doubling the number of operations per instruction.

---

## Swizzled Scale Layout Trace

### ğŸ“¦ FRAME 12: Swizzled Scale Construction

**Test code** ([test_nvfp4_tensor.py:120-132](../../../test/prototype/mx_formats/test_nvfp4_tensor.py#L120)):
```python
def test_nvfp4_swizzled_scales_construction(is_swizzled_scales, shape):
    data = torch.randn(*shape, device="cuda", dtype=torch.bfloat16)
    tensor = NVFP4Tensor.to_nvfp4(data, is_swizzled_scales=is_swizzled_scales)
    assert tensor._is_swizzled_scales == is_swizzled_scales
```

**Swizzled scale layout** for NVFP4:

```
Standard NVFP4 layout (not swizzled):
  Data:  (M, K) â†’ (M, K//2) packed FP4
  Scale: (M, K//16) FP8 E4M3

Swizzled NVFP4 layout:
  Data:  (M, K//2) packed FP4 (unchanged)
  Scale: Blocked layout for CUTLASS

Blocking pattern (128Ã—128 data region):
  - 128 rows of data â†’ 32 rows of scales
  - 128 cols of data (64 cols packed) â†’ 16 cols of scales
  - Interleaved layout for coalesced access
```

**Visual representation**:

```
Input data region: 128Ã—128 elements = 128Ã—64 packed bytes
  â†“
Blockwise scales: 128 rows / 16 elements = 8 scale rows
                  128 cols / 16 elements = 8 scale cols
  â†“
After swizzling: 32Ã—16 scale tile

Linear layout (8Ã—8 scales):
â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”
â”‚ 0â”‚ 1â”‚ 2â”‚ 3â”‚ 4â”‚ 5â”‚ 6â”‚ 7â”‚
â”‚ 8â”‚ 9â”‚10â”‚11â”‚12â”‚13â”‚14â”‚15â”‚
â”‚16â”‚17â”‚18â”‚19â”‚20â”‚21â”‚22â”‚23â”‚
â”‚24â”‚25â”‚26â”‚27â”‚28â”‚29â”‚30â”‚31â”‚
â”‚32â”‚33â”‚34â”‚35â”‚36â”‚37â”‚38â”‚39â”‚
â”‚40â”‚41â”‚42â”‚43â”‚44â”‚45â”‚46â”‚47â”‚
â”‚48â”‚49â”‚50â”‚51â”‚52â”‚53â”‚54â”‚55â”‚
â”‚56â”‚57â”‚58â”‚59â”‚60â”‚61â”‚62â”‚63â”‚
â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜

Swizzled layout (32Ã—16, interleaved):
â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”
â”‚ 0â”‚ 8â”‚16â”‚24â”‚32â”‚40â”‚48â”‚56â”‚ 1â”‚ 9â”‚17â”‚25â”‚33â”‚41â”‚49â”‚57â”‚ â† First 16 scales
â”‚ 2â”‚10â”‚18â”‚26â”‚34â”‚42â”‚50â”‚58â”‚ 3â”‚11â”‚19â”‚27â”‚35â”‚43â”‚51â”‚59â”‚ â† Next 16 scales
â”‚ 4â”‚12â”‚20â”‚28â”‚36â”‚44â”‚52â”‚60â”‚ 5â”‚13â”‚21â”‚29â”‚37â”‚45â”‚53â”‚61â”‚
â”‚ 6â”‚14â”‚22â”‚30â”‚38â”‚46â”‚54â”‚62â”‚ 7â”‚15â”‚23â”‚31â”‚39â”‚47â”‚55â”‚63â”‚
â”‚...                                               â”‚
â”‚(32 rows total, padded for 128-row data regions) â”‚
â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜
```

This layout matches CUTLASS's expected scale format for block-scaled tensor operations.

---

## Summary of Execution Flows

### NVFP4 Quantization (High Precision â†’ NVFP4)

```
User: x_nvfp4 = NVFP4Tensor.to_nvfp4(x, per_tensor_scale=scale)
  â†“
ğŸ“¦ Frame 1: NVFP4Tensor.to_nvfp4() - Public API
  â”œâ”€ Choose backend: Triton vs PyTorch
  â””â”€ Construct NVFP4Tensor wrapper
     â†“
ğŸ“¦ Frame 2: nvfp4_quantize() - PyTorch quantization
  â”œâ”€ Apply per-tensor scale: x / scale
  â”œâ”€ Reshape to blocks (block_size=16)
  â”œâ”€ Compute blockwise scales (FP8 E4M3): amax / FP4_MAX
  â”œâ”€ Normalize: x / blockwise_scale
  â”œâ”€ Quantize to FP4: f32_to_f4_unpacked()
  â”œâ”€ Pack FP4: 2 values per byte
  â””â”€ Optional swizzle scales: to_blocked()
     â†“
OR
ğŸ“¦ Frame 5-7: Triton quantization path
  â”œâ”€ Launch quantize_nvfp4_triton_kernel
  â”œâ”€ Load 128Ã—64 tile
  â”œâ”€ Compute scales with PTX: cvt.e4m3x2.f32
  â”œâ”€ Quantize data with PTX: cvt.e2m1x2.f32
  â””â”€ Store packed FP4 + swizzled scales
     â†“
Returns: NVFP4Tensor(qdata, scale, per_tensor_scale)
```

### NVFP4 Dequantization (NVFP4 â†’ High Precision)

```
User: x_hp = x_nvfp4.dequantize(torch.bfloat16)
  â†“
ğŸ“¦ Frame 4: NVFP4Tensor.dequantize()
  â”œâ”€ Unswizzle scales if needed: from_blocked()
  â”œâ”€ Unpack FP4: unpack_uint4()
  â”œâ”€ Convert FP4 â†’ FP32: f4_unpacked_to_f32()
  â”œâ”€ Reshape to blocks
  â”œâ”€ Apply blockwise scales: x * scale_e4m3
  â”œâ”€ Apply per-tensor scale: x * per_tensor_scale
  â””â”€ Reshape back
     â†“
Returns: torch.Tensor (high precision)
```

### NVFP4 Matrix Multiplication

```
User: C = F.linear(A_nvfp4, B_nvfp4, bias)
  â†“
ğŸ“¦ Frame 8: nvfp4_linear() dispatch
  â”œâ”€ Check mode: DYNAMIC vs WEIGHT_ONLY
  â”œâ”€ DYNAMIC: Quantize activation if needed
  â””â”€ Call nvfp4_mm()
     â†“
ğŸ“¦ Frame 9: nvfp4_mm()
  â”œâ”€ Handle shape broadcasting
  â””â”€ Call _addmm_nvfp4_dispatch()
     â†“
ğŸ“¦ Frame 10: _addmm_nvfp4_dispatch()
  â”œâ”€ Extract scales (blockwise + per-tensor)
  â”œâ”€ Compute output scale
  â””â”€ Call torch._scaled_mm()
     â†“
ğŸ“¦ Frame 11: torch._scaled_mm - CUDA kernel
  â”œâ”€ Load packed FP4 data
  â”œâ”€ Load FP8 E4M3 scales
  â”œâ”€ Blackwell Tensor Core GEMM
  â”‚  â”œâ”€ Unpack FP4 on-the-fly
  â”‚  â”œâ”€ FP4 Ã— FP4 â†’ FP32 accumulate
  â”‚  â””â”€ Apply scales in epilogue
  â””â”€ Store FP32 result
     â†“
Returns: torch.Tensor (FP32 or BF16 output)
```

---

## Key Takeaways

1. **Two-level scaling**: NVFP4 uses per-tensor (coarse) + blockwise (fine) scales for better accuracy

2. **FP8 E4M3 scales**: Unlike MX's E8M0, NVFP4 uses FP8 E4M3 for scales (has mantissa, better granularity)

3. **Fixed block size**: 16 elements per block (vs MX's configurable 32)

4. **Triton with PTX**: Uses inline PTX assembly for hardware-accelerated FP4/FP8 conversion on SM100+

5. **Swizzled scales**: Blocked layout (32Ã—16 tiles) for CUTLASS/tensor core efficiency

6. **torch._scaled_mm**: Native PyTorch support for block-scaled FP4 GEMM on Blackwell

7. **4Ã— throughput**: FP4 Tensor Cores on Blackwell achieve ~4Ã— vs FP8 due to higher density

8. **SQNR**: Achieves â‰¥8 dB reconstruction, â‰¥16 dB matmul (sufficient for inference)

NVFP4 is specifically optimized for NVIDIA's Blackwell architecture, providing state-of-the-art performance for 4-bit quantized inference!
