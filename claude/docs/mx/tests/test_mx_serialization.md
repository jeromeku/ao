# test_mx_serialization.py - Execution Trace Walkthrough

## Overview

[test_mx_serialization.py](../../../test/prototype/mx_formats/test_mx_serialization.py) validates that MX and NVFP4 quantized models can be saved and loaded using PyTorch's standard serialization mechanisms. This is critical for:

- **Model deployment**: Save quantized model ‚Üí Load in production
- **Checkpoint compatibility**: Ensure quantized tensors survive `torch.save` / `torch.load`
- **Weights-only loading**: Security feature that prevents arbitrary code execution
- **Module import isolation**: Verify minimal imports needed for loading

## Test Summary

| Test | Lines | Recipe | Purpose |
|------|-------|--------|---------|
| `test_serialization` | 36-73 | mxfp8 | Save/load MXFP8 model with weights-only=True |
| `test_serialization` | 36-73 | nvfp4 | Save/load NVFP4 model with weights-only=True |

---

## Detailed Execution Trace

---

## test_serialization Execution Trace

**Test code** ([test_mx_serialization.py:36-73](../../../test/prototype/mx_formats/test_mx_serialization.py#L36)):
```python
@pytest.mark.parametrize("recipe_name", ["mxfp8", "nvfp4"])
def test_serialization(recipe_name):
    """
    Ensure that only `import torchao.prototype.mx_formats` is needed
    to load MX and NV checkpoints.
    """

    # Create and quantize a simple model
    m = nn.Linear(32, 128, bias=False, dtype=torch.bfloat16, device="cuda")

    if recipe_name == "mxfp8":
        config = MXFPInferenceConfig(
            activation_dtype=torch.float8_e4m3fn,
            weight_dtype=torch.float8_e4m3fn,
            gemm_kernel_choice=MXGemmKernelChoice.EMULATED,
        )
    else:
        config = NVFP4InferenceConfig(
            mm_config=NVFP4MMConfig.DYNAMIC,
            use_triton_kernel=False,
            use_dynamic_per_tensor_scale=False,
        )

    # Quantize the model
    quantize_(m, config=config)

    # Save to temporary file
    fname = None
    with tempfile.NamedTemporaryFile(delete=False, mode="w") as f:
        torch.save(m.state_dict(), f.name)
        fname = f.name

    # Verify weights-only loading in subprocess
    code = f"""
import torch
import torchao.prototype.mx_formats
_ = torch.load('{fname}', weights_only=True)
    """

    subprocess_out = subprocess.run(["python"], input=code, text=True)
    os.remove(fname)

    assert subprocess_out.returncode == 0, "failed weights-only load"
```

This test validates three critical aspects:
1. **Quantized tensors are serializable** using `torch.save`
2. **Weights-only loading works** (security feature)
3. **Minimal imports required** (only `torchao.prototype.mx_formats`)

---

### üì¶ FRAME 1: Model Creation and Quantization

üìç **Source**: Test setup

**What happens**: Create a simple linear layer and quantize it using one of the MX recipes.

```python
# Create model
m = nn.Linear(32, 128, bias=False, dtype=torch.bfloat16, device="cuda")
# Parameters: weight (128, 32) = 4096 elements

# For MXFP8:
config = MXFPInferenceConfig(
    activation_dtype=torch.float8_e4m3fn,
    weight_dtype=torch.float8_e4m3fn,
    gemm_kernel_choice=MXGemmKernelChoice.EMULATED,
)

# For NVFP4:
config = NVFP4InferenceConfig(
    mm_config=NVFP4MMConfig.DYNAMIC,
    use_triton_kernel=False,
    use_dynamic_per_tensor_scale=False,
)

# Quantize
quantize_(m, config=config)
```

**After quantization**, the model's weight parameter is replaced:

**MXFP8 case**:
```python
# Before: m.weight is torch.Tensor (128, 32) BF16 = 8192 bytes
# After:  m.weight is MXTensor
#   - qdata: (128, 32) FP8 = 4096 bytes
#   - scale: (128, 1) E8M0 = 128 bytes
#   Total: 4224 bytes (48% of original)
```

**NVFP4 case**:
```python
# Before: m.weight is torch.Tensor (128, 32) BF16 = 8192 bytes
# After:  m.weight is NVFP4Tensor
#   - qdata: (128, 16) uint8 packed FP4 = 2048 bytes
#   - scale: (128, 2) FP8 E4M3 = 256 bytes
#   - per_tensor_scale: 1 float32 = 4 bytes
#   Total: 2308 bytes (28% of original)
```

**Next**: ‚Üí Save model in Frame 2

---

### üì¶ FRAME 2: torch.save() - Serialization

üìç **Source**: PyTorch serialization

**What happens**: `torch.save` serializes the model's state dictionary, which triggers custom serialization for MX tensors.

```python
torch.save(m.state_dict(), filename)
```

**Under the hood**, `torch.save` calls `__getstate__` or `__tensor_flatten__` on each parameter.

---

### üì¶ FRAME 3: MXTensor.__tensor_flatten__()

üìç **Source**: [mx_tensor.py:826-838](../../../torchao/prototype/mx_formats/mx_tensor.py#L826)

**What happens**: MXTensor implements the `__tensor_flatten__` protocol for serialization.

```python
def __tensor_flatten__(self) -> tuple[list[str], dict]:
    """
    Flatten MXTensor into saveable components.

    Returns:
        (tensor_names, metadata) where:
        - tensor_names: List of attribute names that are tensors
        - metadata: Dict of serializable metadata
    """

    # List all tensor attributes to save
    tensor_attrs = ["qdata", "scale"]

    # Collect metadata (non-tensor attributes)
    metadata = {
        "_elem_dtype": self._elem_dtype,
        "_block_size": self._block_size,
        "_orig_dtype": self._orig_dtype,
        "_gemm_kernel_choice": self._gemm_kernel_choice,
        "_pack_fp6": self._pack_fp6,
        "_act_quant_kwargs": self._act_quant_kwargs,
        "_is_swizzled_scales": self._is_swizzled_scales,
    }

    return tensor_attrs, metadata
```

**Result**: The MXTensor is decomposed into:
- **Tensors**: `qdata`, `scale` (saved as raw tensor data)
- **Metadata**: Config flags (saved as Python primitives)

**Example for MXFP8 weight**:
```python
tensor_attrs = ["qdata", "scale"]
metadata = {
    "_elem_dtype": torch.float8_e4m3fn,
    "_block_size": 32,
    "_orig_dtype": torch.bfloat16,
    "_gemm_kernel_choice": MXGemmKernelChoice.EMULATED,
    "_pack_fp6": False,
    "_act_quant_kwargs": None,
    "_is_swizzled_scales": False,
}
```

---

### üì¶ FRAME 4: NVFP4Tensor.__tensor_flatten__()

üìç **Source**: [nvfp4_tensor.py:427-443](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L427)

**What happens**: Similar protocol for NVFP4Tensor.

```python
def __tensor_flatten__(self) -> tuple[list[str], dict]:
    """Flatten NVFP4Tensor for serialization."""

    # Tensor attributes
    tensor_attrs = ["qdata", "scale"]

    # Optional per-tensor scales
    if self.per_tensor_scale is not None:
        tensor_attrs.append("per_tensor_scale")
    if self.act_per_tensor_scale is not None:
        tensor_attrs.append("act_per_tensor_scale")

    # Metadata
    metadata = {
        "dtype": str(self.dtype),
        "_block_size": self._block_size,
        "use_triton_kernel": self.use_triton_kernel,
        "act_quant_kwargs": self.act_quant_kwargs,
        "_is_swizzled_scales": self._is_swizzled_scales,
    }

    return tensor_attrs, metadata
```

**Example for NVFP4 weight**:
```python
tensor_attrs = ["qdata", "scale", "per_tensor_scale"]
metadata = {
    "dtype": "torch.bfloat16",
    "_block_size": 16,
    "use_triton_kernel": False,
    "act_quant_kwargs": None,
    "_is_swizzled_scales": True,
}
```

---

### üì¶ FRAME 5: Pickle Serialization

üìç **Source**: Python pickle + PyTorch tensor serialization

**What happens**: PyTorch saves the flattened tensors and metadata using pickle.

**Saved file structure** (conceptual):
```
checkpoint.pth:
  {
    'weight': {
      '__tensor_type__': 'MXTensor',  # or 'NVFP4Tensor'
      'tensors': {
        'qdata': <tensor_bytes>,
        'scale': <tensor_bytes>,
        'per_tensor_scale': <tensor_bytes>,  # NVFP4 only
      },
      'metadata': {
        '_elem_dtype': torch.float8_e4m3fn,
        '_block_size': 32,
        ...
      }
    }
  }
```

üéØ **Key point**: The tensor type information is preserved, allowing reconstruction on load.

---

### üì¶ FRAME 6: Subprocess Load - torch.load()

üìç **Source**: Separate Python process

**What happens**: A fresh Python process attempts to load the checkpoint with **weights_only=True**.

```python
# Fresh Python interpreter (no model code loaded)
import torch
import torchao.prototype.mx_formats  # Register custom tensor types

# Load with security flag
state_dict = torch.load('checkpoint.pth', weights_only=True)
```

**weights_only=True** enforces:
- ‚ùå No arbitrary Python objects (classes, functions)
- ‚ùå No user-defined `__reduce__` methods
- ‚úÖ Only registered tensor types
- ‚úÖ Only primitive Python types (str, int, dict, etc.)

This prevents **pickle exploits** where malicious checkpoints execute arbitrary code.

---

### üì¶ FRAME 7: MXTensor.__tensor_unflatten__()

üìç **Source**: [mx_tensor.py:840-872](../../../torchao/prototype/mx_formats/mx_tensor.py#L840)

**What happens**: PyTorch calls `__tensor_unflatten__` to reconstruct MXTensor from saved components.

```python
@staticmethod
def __tensor_unflatten__(
    inner_tensors: dict[str, torch.Tensor],
    metadata: dict,
    outer_size,
    outer_stride,
) -> "MXTensor":
    """
    Reconstruct MXTensor from flattened representation.

    Args:
        inner_tensors: Dict with 'qdata' and 'scale' tensors
        metadata: Dict with configuration
        outer_size: Shape (unused for MXTensor)
        outer_stride: Strides (unused for MXTensor)

    Returns:
        Reconstructed MXTensor
    """

    # Extract tensors
    qdata = inner_tensors["qdata"]
    scale = inner_tensors["scale"]

    # Extract metadata
    elem_dtype = metadata["_elem_dtype"]
    block_size = metadata["_block_size"]
    orig_dtype = metadata["_orig_dtype"]
    gemm_kernel_choice = metadata["_gemm_kernel_choice"]
    pack_fp6 = metadata["_pack_fp6"]
    act_quant_kwargs = metadata["_act_quant_kwargs"]
    is_swizzled_scales = metadata["_is_swizzled_scales"]

    # Reconstruct MXTensor
    return MXTensor(
        qdata=qdata,
        scale=scale,
        elem_dtype=elem_dtype,
        block_size=block_size,
        orig_dtype=orig_dtype,
        gemm_kernel_choice=gemm_kernel_choice,
        pack_fp6=pack_fp6,
        act_quant_kwargs=act_quant_kwargs,
        is_swizzled_scales=is_swizzled_scales,
    )
```

**Process**:
1. `torch.load` reads pickled data
2. Recognizes `MXTensor` type (registered via `import torchao.prototype.mx_formats`)
3. Calls `MXTensor.__tensor_unflatten__`
4. Passes saved tensors and metadata
5. Reconstructs MXTensor instance

---

### üì¶ FRAME 8: NVFP4Tensor.__tensor_unflatten__()

üìç **Source**: [nvfp4_tensor.py:445-479](../../../torchao/prototype/mx_formats/nvfp4_tensor.py#L445)

**What happens**: Similar reconstruction for NVFP4Tensor.

```python
@staticmethod
def __tensor_unflatten__(
    inner_tensors: dict[str, torch.Tensor],
    metadata: dict,
    outer_size,
    outer_stride,
) -> "NVFP4Tensor":
    """Reconstruct NVFP4Tensor from flattened representation."""

    # Extract tensors
    qdata = inner_tensors["qdata"]
    scale = inner_tensors["scale"]
    per_tensor_scale = inner_tensors.get("per_tensor_scale", None)
    act_per_tensor_scale = inner_tensors.get("act_per_tensor_scale", None)

    # Extract metadata
    dtype = getattr(torch, metadata["dtype"].split(".")[1])
    block_size = metadata["_block_size"]
    use_triton_kernel = metadata["use_triton_kernel"]
    act_quant_kwargs = metadata["act_quant_kwargs"]
    is_swizzled_scales = metadata["_is_swizzled_scales"]

    # Reconstruct NVFP4Tensor
    return NVFP4Tensor(
        qdata=qdata,
        scale=scale,
        dtype=dtype,
        per_tensor_scale=per_tensor_scale,
        act_per_tensor_scale=act_per_tensor_scale,
        use_triton_kernel=use_triton_kernel,
        act_quant_kwargs=act_quant_kwargs,
        is_swizzled_scales=is_swizzled_scales,
    )
```

---

### üì¶ FRAME 9: Tensor Type Registration

üìç **Source**: [__init__.py](../../../torchao/prototype/mx_formats/__init__.py)

**What happens**: Importing `torchao.prototype.mx_formats` registers the custom tensor types.

```python
# torchao/prototype/mx_formats/__init__.py

# Register MXTensor with PyTorch
from torchao.prototype.mx_formats.mx_tensor import MXTensor
torch._C._dispatch_register_torch_dispatch_class(MXTensor)

# Register NVFP4Tensor with PyTorch
from torchao.prototype.mx_formats.nvfp4_tensor import NVFP4Tensor
torch._C._dispatch_register_torch_dispatch_class(NVFP4Tensor)

# This allows torch.load to recognize and reconstruct these types
```

**Why this matters**:
- Without registration, `torch.load` would fail with "unknown tensor type"
- With registration, PyTorch knows how to call `__tensor_unflatten__`
- **Single import** is all that's needed: `import torchao.prototype.mx_formats`

---

## Execution Flow Summary

### Save Path (Parent Process)

```
User: torch.save(model.state_dict(), 'checkpoint.pth')
  ‚Üì
üì¶ Frame 1: Model quantization
  ‚îú‚îÄ nn.Linear.weight: Tensor ‚Üí MXTensor or NVFP4Tensor
  ‚îî‚îÄ Parameters replaced with quantized versions
     ‚Üì
üì¶ Frame 2: torch.save()
  ‚îú‚îÄ Iterate over state_dict parameters
  ‚îî‚îÄ For each MX tensor, call __tensor_flatten__
     ‚Üì
üì¶ Frame 3-4: __tensor_flatten__()
  ‚îú‚îÄ Split tensor into: [qdata, scale, ...] + metadata
  ‚îî‚îÄ Return flattened representation
     ‚Üì
üì¶ Frame 5: Pickle serialization
  ‚îú‚îÄ Serialize tensor bytes (qdata, scale)
  ‚îú‚îÄ Serialize metadata (dtypes, config)
  ‚îî‚îÄ Write to file
     ‚Üì
File saved: checkpoint.pth (28-48% of original size)
```

### Load Path (Subprocess)

```
New Python process:
  ‚Üì
User: import torch; import torchao.prototype.mx_formats
  ‚Üì
üì¶ Frame 9: Tensor type registration
  ‚îú‚îÄ Register MXTensor with PyTorch
  ‚îî‚îÄ Register NVFP4Tensor with PyTorch
     ‚Üì
User: state_dict = torch.load('checkpoint.pth', weights_only=True)
  ‚Üì
üì¶ Frame 6: torch.load()
  ‚îú‚îÄ Read pickled data
  ‚îú‚îÄ Verify weights_only=True (no arbitrary code)
  ‚îú‚îÄ Recognize registered tensor types
  ‚îî‚îÄ Call __tensor_unflatten__ for each MX tensor
     ‚Üì
üì¶ Frame 7-8: __tensor_unflatten__()
  ‚îú‚îÄ Extract inner_tensors: {qdata, scale, ...}
  ‚îú‚îÄ Extract metadata: {dtype, block_size, ...}
  ‚îî‚îÄ Reconstruct MXTensor or NVFP4Tensor
     ‚Üì
Returns: state_dict with fully reconstructed MX tensors
  ‚Üì
‚úÖ Load successful! Model ready for inference
```

---

## Key Serialization Concepts

### 1. __tensor_flatten__ / __tensor_unflatten__ Protocol

PyTorch's mechanism for serializing custom tensor subclasses:

```python
class CustomTensor(torch.Tensor):
    def __tensor_flatten__(self):
        # Save: Split into tensors + metadata
        return (["tensor_attr1", "tensor_attr2"], {"config": value})

    @staticmethod
    def __tensor_unflatten__(inner_tensors, metadata, ...):
        # Load: Reconstruct from components
        return CustomTensor(inner_tensors, **metadata)
```

**Benefits**:
- ‚úÖ Compatible with `weights_only=True`
- ‚úÖ Efficient (saves raw tensor bytes)
- ‚úÖ Versioning-friendly (metadata is explicit)

### 2. Weights-Only Loading

Security feature to prevent pickle exploits:

```python
# Dangerous (pre-PyTorch 2.0):
torch.load('checkpoint.pth')  # Can execute arbitrary code!

# Safe (PyTorch 2.0+):
torch.load('checkpoint.pth', weights_only=True)  # Only trusted types
```

**Allowed with weights_only=True**:
- ‚úÖ PyTorch native tensors
- ‚úÖ Registered tensor subclasses (MXTensor, NVFP4Tensor)
- ‚úÖ Python primitives (str, int, dict, list, tuple)

**Blocked**:
- ‚ùå Arbitrary classes
- ‚ùå User-defined `__reduce__`
- ‚ùå Lambdas, functions

### 3. Tensor Type Registration

Makes custom tensors recognizable by PyTorch:

```python
# During import:
torch._C._dispatch_register_torch_dispatch_class(MXTensor)

# Later, during load:
torch.load(...)  # PyTorch knows about MXTensor
```

Without registration:
```
RuntimeError: Could not find a deserializer for tensor type 'MXTensor'
```

---

## Storage Efficiency Comparison

**Original BF16 model**: `nn.Linear(32, 128)`
- Weight: 128 √ó 32 = 4096 elements √ó 2 bytes = **8192 bytes**

**MXFP8 quantized**:
- qdata: 4096 √ó 1 byte = 4096 bytes
- scale: 128 √ó 1 byte = 128 bytes (block_size=32 ‚Üí 4096/32 = 128 scales)
- **Total: 4224 bytes (51.6% of original)**

**NVFP4 quantized**:
- qdata: 4096 / 2 = 2048 bytes (packed FP4)
- scale: 128 √ó 2 √ó 1 byte = 256 bytes (block_size=16, FP8)
- per_tensor_scale: 4 bytes
- **Total: 2308 bytes (28.2% of original)**

**Savings**:
- MXFP8: ~48% reduction
- NVFP4: ~72% reduction

This makes quantized checkpoints much faster to:
- Download over network
- Load from disk
- Store in memory (multiple checkpoints)

---

## Testing Strategy

### Why Subprocess Test?

The test uses a **subprocess** to validate loading:

```python
code = """
import torch
import torchao.prototype.mx_formats
_ = torch.load('checkpoint.pth', weights_only=True)
"""
subprocess.run(["python"], input=code, text=True)
```

**Reason**: Ensures **minimal imports** are sufficient. If the subprocess succeeds, it proves:
1. No hidden dependencies on model code
2. Single `import torchao.prototype.mx_formats` is enough
3. Works in production (fresh Python interpreter)

### Why weights_only=True?

**Security**: Deployed models should always use `weights_only=True` to prevent:
- **Pickle exploits**: Malicious checkpoints executing code
- **Supply chain attacks**: Compromised checkpoints from untrusted sources

By testing with `weights_only=True`, we ensure MX tensors work in **secure production environments**.

---

## Common Serialization Issues (and How MX Avoids Them)

### Issue 1: Custom Classes Not Serializable

**Problem**: User-defined classes in metadata fail with `weights_only=True`

**Solution**: MX uses only **registered enums** and **primitive types** in metadata:
```python
# ‚úÖ Good (primitives and registered enums)
metadata = {
    "_elem_dtype": torch.float8_e4m3fn,  # Built-in dtype
    "_block_size": 32,                    # int
    "_pack_fp6": False,                   # bool
}

# ‚ùå Bad (custom class)
metadata = {
    "config": MyCustomConfig(...),  # Fails weights_only=True
}
```

### Issue 2: State Lost on Load

**Problem**: Reconstructed tensor missing attributes

**Solution**: MX explicitly lists all state in `__tensor_flatten__`:
```python
def __tensor_flatten__(self):
    # List ALL tensor attributes
    tensors = ["qdata", "scale"]

    # Include ALL metadata
    metadata = {
        "_elem_dtype": self._elem_dtype,
        "_block_size": self._block_size,
        # ... every config flag
    }

    return tensors, metadata
```

### Issue 3: Version Incompatibility

**Problem**: Old checkpoints fail to load with new code

**Solution**: MX metadata is **explicit and versioned**:
- Missing keys ‚Üí default values
- Unknown keys ‚Üí ignored
- Dtype changes ‚Üí explicit conversion

---

## Summary

**test_serialization validates**:

1. ‚úÖ **MXTensor and NVFP4Tensor are serializable** using `torch.save`
2. ‚úÖ **Weights-only loading works** (secure, production-ready)
3. ‚úÖ **Minimal imports required** (just `import torchao.prototype.mx_formats`)
4. ‚úÖ **Storage efficient** (28-51% of original size)
5. ‚úÖ **No model code needed** to load (true separation of train/deploy)

This enables:
- üöÄ **Fast model deployment** (small checkpoint files)
- üîí **Secure loading** (weights_only=True)
- üì¶ **Portable checkpoints** (work in any environment)
- üîÑ **Backward compatibility** (explicit metadata versioning)

MX/NVFP4 quantized models integrate seamlessly with PyTorch's serialization ecosystem!
